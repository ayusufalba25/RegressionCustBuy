plot(df)
# Create variables for dependent and independent variables
y <- df$consider_buying
X <- df[, -1]
set.seed(123)
train_set = sample(1:length(y), round(0.8 * length(y), 0))
X_train <- X[train_set,]
y_train <- y[train_set]
X_test <- X[-train_set,]
y_test <- y[-train_set]
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df)
summary(model)
plot(model)
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
# Fitting linear regression model but without product_quality
model <- lm(consider_buying ~ price, data = df_train)
summary(model)
plot(model)
library(MASS)
# Check for multicollinearity using VIF
VIF(model)
install.packages("car")
library(car)
# Check for multicollinearity using VIF
vif(model)
summary(model)
# Normality test for residual
shapiro.test(model$residuals)
install.packages("lmtest")
library(lmtest)
# Heteroscedasticity test
bptest(model)
# Predict test data
df_test[-1]
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
rmse(df_test$consider_buying, y_test_pred)
summary(model)
# MAPE
mape <- function(y_true, y_pred){
return((abs(y_true - y_pred) / y_true) * 100)
}
mape(df_test$consider_buying, y_test_pred)
# MAPE
mape <- function(y_true, y_pred){
return((sum(abs(y_true - y_pred) / y_true) * 100))
}
mape(df_test$consider_buying, y_test_pred)
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
mape(df_test$consider_buying, y_test_pred)
# Check the difference
cbind(df_test$consider_buying. y_test_pred)
# Check the difference
cbind(df_test$consider_buying, y_test_pred)
# Check the difference
cbind(df_test$consider_buying, round(y_test_pred, 0))
library(tidyverse)
library(readxl)
library(skimr)
library(car)
library(lmtest)
plot(cars)
print("hello world!")
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
View(df)
# Check the summary of the data
skim(df)
# Create a scatterplot to check if there is any clear of relationship pattern
plot(df)
install.packages("corrplot")
install.packages("psych")
library(psych)
pairs.panels(df)
pairs.panels(df, method = "spearman")
# Clustering customers
kmeans(df, 3)
# Clustering customers
kmeans(df, 2)
# Clustering customers
cluster_customer <- kmeans(df, 2)
cluster_customer
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
View(df)
df
# Check the summary of the data
skim(df)
# Check if there is any clear of relationship pattern
pairs.panels(df, method = "spearman")
# Split the data into train and test set
set.seed(123)
train_set = sample(1:length(y), round(0.8 * length(y), 0))
length(df)
nrow(df)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.8 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Fitting linear regression model but without product_quality
model <- lm(consider_buying ~ price, data = df_train)
summary(model)
plot(model)
# Model assumption checking
# Normality test for residual
shapiro.test(model$residuals)
# Heteroscedasticity test
bptest(model)
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
rmse(df_test$consider_buying, y_test_pred)
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
mape(df_test$consider_buying, y_test_pred)
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
cat("RMSE = ", rmse(df_test$consider_buying, y_test_pred))
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
cat("MAPE = ", mape(df_test$consider_buying, y_test_pred))
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
cat("RMSE = ", rmse(df_test$consider_buying, y_test_pred), "\n")
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
cat("MAPE = ", mape(df_test$consider_buying, y_test_pred))
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
cat("RMSE =", rmse(df_test$consider_buying, y_test_pred), "\n")
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
cat("MAPE =", mape(df_test$consider_buying, y_test_pred))
# Check the difference
cbind(df_test$consider_buying, round(y_test_pred, 0))
# Clustering customers
cluster_customer <- kmeans(df, 2)
cluster_customer
# Check if there is any outliers in the data using boxplot
boxplot(df$consider_buying)
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
# Check if there is any outliers in the data using boxplot
boxplot(df$consider_buying)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Check the summary of the data
skim(df)
# Check if there is any clear of relationship pattern
pairs.panels(df, method = "spearman")
# Check if there is any outliers in the data using boxplot
boxplot(df$consider_buying)
# Check if there is any outliers in the data using boxplot
boxplot(df$consider_buying)
boxplot(df$price)
boxplot(df$product_quality)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.8 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Check if there is any outliers in the data using boxplot
par(mfrow = c(1, 3))
boxplot(df$consider_buying)
boxplot(df$price)
boxplot(df$product_quality)
# Import libraries
library(tidyverse)
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Skim the data
skim(df)
# Check if there is any clear of relationship pattern
pairs.panels(df, method = "spearman")
# Check if there is any outliers in the data using boxplot
par(mfrow = c(1, 3))
boxplot(df$consider_buying)
boxplot(df$price)
boxplot(df$product_quality)
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.8 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-influential,]
names(influential)
as.numeric(names(influential))
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.8 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Check outliers in the model using Cook's distance
cooksD <- cooks.distance(model)
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
influential
# Remove the outliers from the observations
df_train <- df_train[-as.numeric(names(influential)),]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.9 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.9 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
plot(model)
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
# Fitting linear regression model but without product_quality
model <- lm(consider_buying ~ price, data = df_train)
summary(model)
# Model assumption checking
# Normality test for residual
shapiro.test(model$residuals)
# Heteroscedasticity test
bptest(model)
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
cat("RMSE =", rmse(df_test$consider_buying, y_test_pred), "\n")
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
cat("MAPE =", mape(df_test$consider_buying, y_test_pred))
# Check the difference
cbind(df_test$consider_buying, round(y_test_pred, 0))
# Clustering customers
cluster_customer <- kmeans(df[-3], 2)
cluster_customer
plot(df$price, df$consider_buying)
cluster_customer$cluster
plot(df$price, df$consider_buying, col = cluster_customer$cluster)
plot(df$price, df$consider_buying, col = cluster_customer$cluster)
legend()
plot(df$price, df$consider_buying, col = cluster_customer$cluster, legend = T)
# Add a column to store cluster
df <- df %>% mutate(cluster = cluster_customer$cluster)
df
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying, color = cluster)) +
geom_point()
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying), color = cluster) +
geom_point()
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying, shape = cluster)) +
geom_point()
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying, color = cluster)) +
geom_point()
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying, colour = cut(cluster, c(-Inf, 1.5, Inf)))) +
geom_point()
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying)) +
geom_point(aes(colour = cut(cluster, c(-Inf, 1.5, Inf))))
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying)) +
geom_point(aes(colour = cut(cluster, c(-Inf, 1.5, Inf)))) +
scale_color_manual(name = "Cluster",
labels = c("Cluster 1", "Cluster 2"))
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying)) +
geom_point(aes(colour = cut(cluster, c(-Inf, 1.5, Inf)))) +
scale_color_manual(name = "Cluster",
values = c("(-Inf,1.5]" = "black",
"(1.5, Inf]" = "red"),
labels = c("Cluster 1", "Cluster 2"))
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Add a column to store cluster
df <- df %>% select(-product_quality) %>% mutate(cluster = cluster_customer$cluster)
df
# Create a plot to visualize the cluster
ggplot(df, aes(price, consider_buying)) +
geom_point(aes(colour = cut(cluster, c(-Inf, 1.5, Inf)))) +
scale_color_manual(name = "Cluster",
values = c("(-Inf,1.5]" = "black",
"(1.5, Inf]" = "red"),
labels = c("Cluster 1", "Cluster 2"))
# Add a column to store cluster
df <- df %>% select(-product_quality) %>% mutate(cluster = cluster_customer$cluster)
# Import libraries
library(tidyverse)
library(readxl)
library(skimr)
library(lmtest)
library(psych)
# Import data
df <- read_xlsx("Dapur Putih Cafe.xlsx")
df
# Skim the data
skim(df)
# Check if there is any clear of relationship pattern
pairs.panels(df, method = "spearman")
# Check if there is any outliers in the data using boxplot
par(mfrow = c(1, 3))
boxplot(df$consider_buying)
boxplot(df$price)
boxplot(df$product_quality)
# Split the data into train and test set
set.seed(123)
train_set = sample(1:nrow(df), round(0.9 * nrow(df), 0))
df_train <- df[train_set,]
df_test <- df[-train_set,]
# Fitting linear regression model
model <- lm(consider_buying ~ ., data = df_train)
summary(model)
# Fitting linear regression model but without product_quality
model <- lm(consider_buying ~ price, data = df_train)
summary(model)
# Model assumption checking
# Normality test for residual
shapiro.test(model$residuals)
# Heteroscedasticity test
bptest(model)
# Prediction performance
# Predict test data
y_test_pred <- predict(model, df_test[-1])
# RMSE
rmse <- function(y_true, y_pred){
mse <- mean((y_true - y_pred) ^ 2)
return(sqrt(mse))
}
cat("RMSE =", rmse(df_test$consider_buying, y_test_pred), "\n")
# MAPE
mape <- function(y_true, y_pred){
return((mean(abs(y_true - y_pred) / y_true) * 100))
}
cat("MAPE =", mape(df_test$consider_buying, y_test_pred))
# Clustering customers
cluster_customer <- kmeans(df[-3], 2)
cluster_customer
# Add a column to store cluster
df <- df %>% select(-product_quality) %>% mutate(cluster = cluster_customer$cluster)
df
